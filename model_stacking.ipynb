{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c18b72",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "For: NEIL HEINRICH BRAUN\n",
    "\n",
    "The idea for this file is to train a lightGBM model (or other models) for stacking. The data files you will need to import is unfortunately not ready. For now, imagine a file in the following format\n",
    "\n",
    "| ticker | quarter_year | actual_log_rev | actual_car | log_revenue_prediction_1 | CAR_prediction_1 | log_revenue_prediction_2 | CAR_prediction_2 |\n",
    "| ------ | ------------ | -------------- | ---------- | ------------------------ | ---------------- | ------------------------ | ---------------- |\n",
    "| BAC    | Q1 2001      | 123            | 0.4        | 123                      | 0.5              | 123                      | 0.5              |\n",
    "| JPM    | Q1 2001      | 123            | 0.4        | 456                      | 0.8              | 456                      | 0.8              |\n",
    "| WFC    | Q1 2001      | 123            | 0.4        | 789                      | 0.25             | 789                      | 0.25             |\n",
    "\n",
    "There will be more columns where each column will be the prediction from each previous model for each y value.\n",
    "\n",
    "You will need to train at least 2 models for stacking. 1 for each y value. Of course try other things outside of LightGBM as well but I suspect GBM to be the best. If you try other things outside of GBM, process the data accordingly to get a good range.Your output should be in the format:\n",
    "\n",
    "| ticker | quarter_year | log_revenue_prediction | CAR_prediction |\n",
    "| ------ | ------------ | ---------------------- | -------------- |\n",
    "| BAC    | Q1 2001      | 123                    | 0.5            |\n",
    "| JPM    | Q1 2001      | 456                    | 0.8            |\n",
    "| WFC    | Q1 2001      | 789                    | 0.25           |\n",
    "\n",
    "Enjoy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e070bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create directory for models if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the stacking data from a CSV file\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Loaded DataFrame\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Print some basic information about the data\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"Number of unique companies: {df['ticker'].nunique()}\")\n",
    "    print(f\"Time period: {df['quarter_year'].min()} to {df['quarter_year'].max()}\")\n",
    "    \n",
    "    # Create a numerical time_id from quarter_year for time-based splitting\n",
    "    df['quarter'] = df['quarter_year'].str.extract(r'Q(\\d)').astype(int)\n",
    "    df['year'] = df['quarter_year'].str.extract(r'(\\d{4})').astype(int)\n",
    "    df['time_id'] = df['year'] * 4 + df['quarter']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_for_stacking(\n",
    "    df: pd.DataFrame, \n",
    "    target_column: str, \n",
    "    id_columns: List[str] = ['ticker', 'quarter_year', 'time_id', 'quarter', 'year']\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Prepare features for stacking model\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with predictions from base models\n",
    "        target_column: Name of the target column (actual values)\n",
    "        id_columns: List of identifier columns to exclude from features\n",
    "        \n",
    "    Returns:\n",
    "        X and y for training\n",
    "    \"\"\"\n",
    "    # Filter columns that contain predictions (feature columns)\n",
    "    feature_columns = [\n",
    "        col for col in df.columns \n",
    "        if col not in id_columns and col != target_column and col != 'actual_log_rev' and col != 'actual_car'\n",
    "    ]\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = df[feature_columns].copy()\n",
    "    \n",
    "    # Create target vector\n",
    "    y = df[target_column].copy()\n",
    "    \n",
    "    print(f\"Features for stacking: {feature_columns}\")\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(df: pd.DataFrame, n_splits: int = 3) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Create time-based cross-validation splits\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to split\n",
    "        n_splits: Number of splits (reduced to 3 for smaller dataset)\n",
    "        \n",
    "    Returns:\n",
    "        List of train and validation indices\n",
    "    \"\"\"\n",
    "    # Sort by time\n",
    "    df = df.sort_values('time_id')\n",
    "    \n",
    "    # Create time-based splits with reduced number of splits\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(df):\n",
    "        splits.append((train_idx, val_idx))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cab546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_stacking_model(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series, \n",
    "    cv_splits: List[Tuple[np.ndarray, np.ndarray]], \n",
    "    params: Optional[Dict] = None\n",
    ") -> Tuple[lgb.Booster, float]:\n",
    "    \"\"\"\n",
    "    Train a LightGBM stacking model with cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target variable\n",
    "        cv_splits: Cross-validation splits\n",
    "        params: LightGBM parameters\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and average validation score\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 7,  # Reduced for smaller dataset\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    # Keep track of validation scores\n",
    "    val_scores = []\n",
    "    fold_models = []\n",
    "    \n",
    "    # Train model with each fold\n",
    "    for i, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Create LightGBM datasets\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        # Train model\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=20, verbose=False),  # Reduced for smaller dataset\n",
    "            lgb.log_evaluation(period=0)  # Disable verbose output\n",
    "        ]\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params, \n",
    "            train_data,\n",
    "            valid_sets=[train_data, val_data],\n",
    "            valid_names=['train', 'valid'],\n",
    "            num_boost_round=200,  # Reduced for smaller dataset\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        val_preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        r2 = r2_score(y_val, val_preds)\n",
    "        \n",
    "        val_scores.append(rmse)\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        print(f\"Fold {i+1}/{len(cv_splits)}: RMSE = {rmse:.4f}, RÂ² = {r2:.4f}\")\n",
    "    \n",
    "    # Find the best model\n",
    "    best_idx = np.argmin(val_scores)\n",
    "    best_model = fold_models[best_idx]\n",
    "    \n",
    "    # Calculate average validation score\n",
    "    avg_rmse = np.mean(val_scores)\n",
    "    print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "    \n",
    "    # Plot feature importance for the best model\n",
    "    try:\n",
    "        feature_importance = best_model.feature_importance()\n",
    "        feature_names = X.columns.tolist()\n",
    "        \n",
    "        # Create a DataFrame for the feature importance\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "        plt.title(f'Feature Importance (Best Model)')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance.png')\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot feature importance: {e}\")\n",
    "    \n",
    "    return best_model, avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fe6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_alternative_stacking_models(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series, \n",
    "    cv_splits: List[Tuple[np.ndarray, np.ndarray]]\n",
    ") -> Dict[str, Tuple[object, float]]:\n",
    "    \"\"\"\n",
    "    Train alternative stacking models\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target variable\n",
    "        cv_splits: Cross-validation splits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models and their scores\n",
    "    \"\"\"\n",
    "    # Define models to try (with parameters optimized for smaller datasets)\n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42, alpha=0.1, l1_ratio=0.5, max_iter=1000),\n",
    "        'Ridge': Ridge(random_state=42, alpha=1.0, max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    results = {}\n",
    "    \n",
    "    # Standardize features for linear models\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Train each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        val_scores = []\n",
    "        \n",
    "        try:\n",
    "            for i, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "                if name in ['ElasticNet', 'Ridge']:\n",
    "                    # Use scaled features for linear models\n",
    "                    X_train, y_train = X_scaled.iloc[train_idx], y.iloc[train_idx]\n",
    "                    X_val, y_val = X_scaled.iloc[val_idx], y.iloc[val_idx]\n",
    "                else:\n",
    "                    # Use original features for tree-based models\n",
    "                    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "                    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                val_preds = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "                r2 = r2_score(y_val, val_preds)\n",
    "                \n",
    "                val_scores.append(rmse)\n",
    "                print(f\"Fold {i+1}/{len(cv_splits)}: RMSE = {rmse:.4f}, RÂ² = {r2:.4f}\")\n",
    "            \n",
    "            # Calculate average validation score\n",
    "            avg_rmse = np.mean(val_scores)\n",
    "            print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "            \n",
    "            # Store the model and its score\n",
    "            if name in ['ElasticNet', 'Ridge']:\n",
    "                # Also store the scaler for linear models\n",
    "                results[name] = (model, avg_rmse, scaler)\n",
    "            else:\n",
    "                results[name] = (model, avg_rmse)\n",
    "            \n",
    "            # Try to plot feature importance if available\n",
    "            try:\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    importance = model.feature_importances_\n",
    "                    feature_names = X.columns.tolist()\n",
    "                    \n",
    "                    # Create a DataFrame for the feature importance\n",
    "                    importance_df = pd.DataFrame({\n",
    "                        'Feature': feature_names,\n",
    "                        'Importance': importance\n",
    "                    }).sort_values('Importance', ascending=False)\n",
    "                    \n",
    "                    # Plot feature importance\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "                    plt.title(f'Feature Importance ({name})')\n",
    "                    plt.xlabel('Importance')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(f'feature_importance_{name}.png')\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not plot feature importance for {name}: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3070d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(lightgbm_rmse: float, alternative_models: Dict[str, Tuple[object, float]]) -> Tuple[str, object, float]:\n",
    "    \"\"\"\n",
    "    Find the best model based on RMSE\n",
    "    \n",
    "    Args:\n",
    "        lightgbm_rmse: RMSE for LightGBM model\n",
    "        alternative_models: Dictionary of alternative models and their scores\n",
    "        \n",
    "    Returns:\n",
    "        Name of the best model, the model itself, and its RMSE\n",
    "    \"\"\"\n",
    "    # Compare LightGBM with alternative models\n",
    "    best_rmse = lightgbm_rmse\n",
    "    best_model_name = 'LightGBM'\n",
    "    \n",
    "    for name, model_info in alternative_models.items():\n",
    "        model_rmse = model_info[1]\n",
    "        if model_rmse < best_rmse:\n",
    "            best_rmse = model_rmse\n",
    "            best_model_name = name\n",
    "    \n",
    "    # Return the best model\n",
    "    if best_model_name == 'LightGBM':\n",
    "        print(f\"\\nBest model: {best_model_name} (RMSE: {best_rmse:.4f})\")\n",
    "        return best_model_name, None, best_rmse\n",
    "    else:\n",
    "        model = alternative_models[best_model_name][0]\n",
    "        print(f\"\\nBest model: {best_model_name} (RMSE: {best_rmse:.4f})\")\n",
    "        return best_model_name, model, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run the full stacking pipeline\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with predictions from base models\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with stacked predictions\n",
    "    \"\"\"\n",
    "    # Create time-based CV splits - do this once and reuse\n",
    "    cv_splits = time_series_cv(df, n_splits=3)  # Reduced to 3 splits for smaller dataset\n",
    "    \n",
    "    # Split the data into revenue and CAR prediction tasks\n",
    "    print(\"\\n=== Revenue Stacking ===\")\n",
    "    \n",
    "    # Prepare features for revenue prediction\n",
    "    X_rev, y_rev = prepare_features_for_stacking(df, 'actual_log_rev')\n",
    "    \n",
    "    # Train LightGBM stacking model for revenue\n",
    "    lightgbm_rev, lightgbm_rmse_rev = train_lightgbm_stacking_model(X_rev, y_rev, cv_splits)\n",
    "    \n",
    "    # Save the LightGBM model\n",
    "    joblib.dump(lightgbm_rev, 'models/stacking_lightgbm_revenue.pkl')\n",
    "    \n",
    "    # Train alternative models for revenue\n",
    "    alternative_models_rev = train_alternative_stacking_models(X_rev, y_rev, cv_splits)\n",
    "    \n",
    "    # Find the best model for revenue\n",
    "    best_model_name_rev, best_model_rev, best_rmse_rev = find_best_model(lightgbm_rmse_rev, alternative_models_rev)\n",
    "    \n",
    "    # Save the best alternative model if it's better than LightGBM\n",
    "    if best_model_name_rev != 'LightGBM' and best_model_rev is not None:\n",
    "        joblib.dump(best_model_rev, f'models/stacking_{best_model_name_rev.lower()}_revenue.pkl')\n",
    "    \n",
    "    # Repeat for CAR prediction\n",
    "    print(\"\\n=== CAR Stacking ===\")\n",
    "    \n",
    "    # Prepare features for CAR prediction\n",
    "    X_car, y_car = prepare_features_for_stacking(df, 'actual_car')\n",
    "    \n",
    "    # Train LightGBM stacking model for CAR\n",
    "    lightgbm_car, lightgbm_rmse_car = train_lightgbm_stacking_model(X_car, y_car, cv_splits)\n",
    "    \n",
    "    # Save the LightGBM model\n",
    "    joblib.dump(lightgbm_car, 'models/stacking_lightgbm_car.pkl')\n",
    "    \n",
    "    # Train alternative models for CAR\n",
    "    alternative_models_car = train_alternative_stacking_models(X_car, y_car, cv_splits)\n",
    "    \n",
    "    # Find the best model for CAR\n",
    "    best_model_name_car, best_model_car, best_rmse_car = find_best_model(lightgbm_rmse_car, alternative_models_car)\n",
    "    \n",
    "    # Save the best alternative model if it's better than LightGBM\n",
    "    if best_model_name_car != 'LightGBM' and best_model_car is not None:\n",
    "        joblib.dump(best_model_car, f'models/stacking_{best_model_name_car.lower()}_car.pkl')\n",
    "    \n",
    "    # Generate final predictions\n",
    "    # Use LightGBM models as default, override with better model if available\n",
    "    df['log_revenue_prediction'] = lightgbm_rev.predict(X_rev)\n",
    "    df['CAR_prediction'] = lightgbm_car.predict(X_car)\n",
    "    \n",
    "    # If a better model was found for revenue, use it instead\n",
    "    if best_model_name_rev != 'LightGBM' and best_model_rev is not None:\n",
    "        if best_model_name_rev in ['ElasticNet', 'Ridge']:\n",
    "            # Need to scale the features for linear models\n",
    "            scaler = alternative_models_rev[best_model_name_rev][2]\n",
    "            X_rev_scaled = scaler.transform(X_rev)\n",
    "            df['log_revenue_prediction'] = best_model_rev.predict(X_rev_scaled)\n",
    "        else:\n",
    "            df['log_revenue_prediction'] = best_model_rev.predict(X_rev)\n",
    "    \n",
    "    # If a better model was found for CAR, use it instead\n",
    "    if best_model_name_car != 'LightGBM' and best_model_car is not None:\n",
    "        if best_model_name_car in ['ElasticNet', 'Ridge']:\n",
    "            # Need to scale the features for linear models\n",
    "            scaler = alternative_models_car[best_model_name_car][2]\n",
    "            X_car_scaled = scaler.transform(X_car)\n",
    "            df['CAR_prediction'] = best_model_car.predict(X_car_scaled)\n",
    "        else:\n",
    "            df['CAR_prediction'] = best_model_car.predict(X_car)\n",
    "    \n",
    "    # Return final predictions in the required format\n",
    "    final_predictions = df[['ticker', 'quarter_year', 'log_revenue_prediction', 'CAR_prediction']].copy()\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662000a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the stacking pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the small stacking data\n",
    "        df = load_data('data/stacking_data_expanded_small.csv')\n",
    "        \n",
    "        # Run the stacking pipeline\n",
    "        predictions = stacking_pipeline(df)\n",
    "        \n",
    "        # Save the predictions\n",
    "        predictions.to_csv('stacking_predictions.csv', index=False)\n",
    "        \n",
    "        print(\"\\nStacking pipeline completed successfully!\")\n",
    "        print(\"Predictions saved to 'stacking_predictions.csv'\")\n",
    "        \n",
    "        # Display a sample of the predictions\n",
    "        print(\"\\nSample predictions:\")\n",
    "        print(predictions.head(10))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main pipeline\n",
    "    final_predictions = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
