{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f19cfd",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "For: NEIL HEINRICH BRAUN\n",
    "\n",
    "The idea for this file is to train a lightGBM model given the dataset. The data files you will need to import is unfortunately not ready. But for now, write and test the code using `model_building_data.csv` which is provided in the data folder. Keep in mind that the final training/testing files will have more fields.\n",
    "\n",
    "Great thing about LightGBM is it can handle missing data as is. Also, LightGBM can also handle data with weird ranges better than compared to neural network based models or even models like SVM regressor. So in practice, you dont need to do much processing because the data file should already have appropriate data for you to use.\n",
    "\n",
    "Of course this does not mean you should not do any processing at all. In fact, you should explore dimension reduction techniques and do feature selection where appropriate. Also, some columns might have too many NaNs and should be remove entirely.\n",
    "\n",
    "Furthermore, LightGBM is *not* a timeseries model. Therefore, you should engineer lagged variables for prediction as well.\n",
    "\n",
    "Last thing to keep in mind is, some rows might have missing revenue but non-missing CAR etc. If you will drop NaNs, drop for each y values differently to prevent unnecessary data loss.\n",
    "\n",
    "Tune all parameters using 3-fold CV with the timesplit function like in assignment 1. I'll write a different time split function and we'll rerun with 5-10 fold CV again later before submission.\n",
    "\n",
    "This file should save the output of the prediction in the format:\n",
    "\n",
    "| ticker | quarter_year  | log_revenue_prediction | CAR_prediction |\n",
    "|--------|---------------|------------------------|----------------|\n",
    "| BAC    | Q1 2001       | 123                    | 0.5            |\n",
    "| JPM    | Q1 2001       | 456                    | 0.8            |\n",
    "| WFC    | Q1 2001       | 789                    | 0.25           |\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc986da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create directory for models if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean column names to remove special characters that might cause issues with LightGBM\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with original column names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    # Create a mapping of original to clean column names\n",
    "    column_mapping = {}\n",
    "    for col in df.columns:\n",
    "        # Replace special characters with underscore\n",
    "        clean_col = re.sub(r'[^\\w\\s]', '_', col)\n",
    "        # Replace spaces with underscore\n",
    "        clean_col = re.sub(r'\\s+', '_', clean_col)\n",
    "        # Ensure unique column names\n",
    "        if clean_col in column_mapping.values():\n",
    "            i = 1\n",
    "            while f\"{clean_col}_{i}\" in column_mapping.values():\n",
    "                i += 1\n",
    "            clean_col = f\"{clean_col}_{i}\"\n",
    "        column_mapping[col] = clean_col\n",
    "    \n",
    "    # Create a copy of the DataFrame with cleaned column names\n",
    "    df_clean = df.copy()\n",
    "    df_clean.columns = [column_mapping[col] for col in df.columns]\n",
    "    \n",
    "    # Store the column mapping for later reference\n",
    "    df_clean.attrs['column_mapping'] = column_mapping\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the data from a CSV file and do initial preprocessing\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Parse the quarter from datacqtr\n",
    "    # Example: Convert '2000Q1' to quarter=1, year=2000\n",
    "    df['year'] = df['datacqtr'].str.extract(r'(\\d{4})').astype(int)\n",
    "    df['quarter'] = df['datacqtr'].str.extract(r'Q(\\d)').astype(int)\n",
    "    \n",
    "    # Create a numerical representation of time for sorting\n",
    "    df['time_id'] = df['year'] * 4 + df['quarter']\n",
    "    \n",
    "    # Keep the original quarter_year format for output\n",
    "    df['quarter_year'] = df['datacqtr'].apply(lambda x: f\"Q{x[-1]} {x[:4]}\")\n",
    "    \n",
    "    # Clean column names\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Print some basic information about the data\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"Number of unique companies: {df['tic'].nunique()}\")\n",
    "    print(f\"Time period: {df['datacqtr'].min()} to {df['datacqtr'].max()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df: pd.DataFrame, threshold: float = 0.7) -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Check for missing values in each column and remove columns with too many missing values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check\n",
    "        threshold: Maximum percentage of missing values allowed\n",
    "        \n",
    "    Returns:\n",
    "        List of columns to keep and cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Calculate percentage of missing values in each column\n",
    "    missing_percent = df.isnull().mean()\n",
    "    \n",
    "    # Print columns with missing values\n",
    "    print(\"Columns with missing values:\")\n",
    "    for col in missing_percent[missing_percent > 0].index:\n",
    "        print(f\"{col}: {missing_percent[col]*100:.2f}%\")\n",
    "    \n",
    "    # Identify columns to keep (with missing values below threshold)\n",
    "    # Always keep time_id, tic, and quarter_year columns\n",
    "    must_keep = ['time_id', 'tic', 'quarter_year']\n",
    "    cols_with_low_missing = missing_percent[missing_percent < threshold].index.tolist()\n",
    "    cols_to_keep = list(set(must_keep + cols_with_low_missing))\n",
    "    \n",
    "    # Remove columns with too many missing values\n",
    "    df_cleaned = df[cols_to_keep].copy()\n",
    "    \n",
    "    return cols_to_keep, df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df: pd.DataFrame, lag_columns: List[str], lag_periods: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create lagged features for specified columns\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to create lagged features for\n",
    "        lag_columns: List of columns to create lags for\n",
    "        lag_periods: List of lag periods\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with lagged features\n",
    "    \"\"\"\n",
    "    # Sort the data by ticker and time\n",
    "    df = df.sort_values(['tic', 'time_id'])\n",
    "    \n",
    "    # Create lagged features\n",
    "    for col in lag_columns:\n",
    "        if col in df.columns:  # Only process columns that exist\n",
    "            for lag in lag_periods:\n",
    "                lag_col_name = f\"{col}_lag_{lag}\"\n",
    "                df[lag_col_name] = df.groupby('tic')[col].shift(lag)\n",
    "    \n",
    "    # Calculate rate of change features\n",
    "    for col in lag_columns:\n",
    "        if col in df.columns:  # Only process columns that exist\n",
    "            for lag in lag_periods:\n",
    "                if lag > 0:\n",
    "                    # Check if lag column exists\n",
    "                    lag_col_name = f\"{col}_lag_{lag}\"\n",
    "                    if lag_col_name in df.columns:\n",
    "                        # Use safe division to avoid divide by zero issues\n",
    "                        roc_col_name = f\"{col}_roc_{lag}\"\n",
    "                        df[roc_col_name] = df.groupby('tic').apply(\n",
    "                            lambda x: (x[col] - x[lag_col_name]) / \n",
    "                            x[lag_col_name].replace(0, np.nan)\n",
    "                        ).reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Calculate rolling means\n",
    "    for col in lag_columns:\n",
    "        if col in df.columns:  # Only process columns that exist\n",
    "            for window in [2, 4]:\n",
    "                roll_col_name = f\"{col}_rolling_mean_{window}\"\n",
    "                df[roll_col_name] = df.groupby('tic')[col].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b009b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(df: pd.DataFrame, target_col: str, drop_cols: List[str]) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Prepare data for training by separating features and target\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to prepare\n",
    "        target_col: Name of the target column\n",
    "        drop_cols: List of columns to drop from features\n",
    "        \n",
    "    Returns:\n",
    "        X and y for training\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Columns to drop from features\n",
    "    cols_to_drop = drop_cols.copy()\n",
    "    if target_col not in cols_to_drop:\n",
    "        cols_to_drop.append(target_col)\n",
    "    \n",
    "    # Map revenue and CAR column names\n",
    "    revenue_col = [col for col in df.columns if 'Total_Current_Operating_Revenue' in col]\n",
    "    car_col = [col for col in df.columns if 'car5' in col]\n",
    "    \n",
    "    # Drop target-related columns\n",
    "    if revenue_col and target_col not in revenue_col:\n",
    "        cols_to_drop.extend(revenue_col)\n",
    "    if car_col and target_col not in car_col:\n",
    "        cols_to_drop.extend(car_col)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Handle categorical features\n",
    "    cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    for col in cat_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_model(X: pd.DataFrame, y: pd.Series, n_splits: int = 3, \n",
    "                        params: Optional[Dict] = None) -> Tuple[lgb.Booster, float, Dict]:\n",
    "    \"\"\"\n",
    "    Train a LightGBM model with time series cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target variable\n",
    "        n_splits: Number of splits for cross-validation\n",
    "        params: LightGBM parameters\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, average validation score, and feature importances\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    # Create time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    # Keep track of validation scores\n",
    "    val_scores = []\n",
    "    models = []\n",
    "    feature_importances = {}\n",
    "    \n",
    "    # Initialize feature importances\n",
    "    for feature in X.columns:\n",
    "        feature_importances[feature] = 0\n",
    "    \n",
    "    # Select index (time_id if available, or use range index)\n",
    "    if 'time_id' in X.columns:\n",
    "        sorted_indices = X.sort_values('time_id').index\n",
    "    else:\n",
    "        sorted_indices = np.arange(len(X))\n",
    "    \n",
    "    # Train model with each fold\n",
    "    for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        # Map indices back to original dataframe indices\n",
    "        if len(train_idx) > 0 and len(val_idx) > 0:\n",
    "            try:\n",
    "                actual_train_idx = sorted_indices[train_idx]\n",
    "                actual_val_idx = sorted_indices[val_idx]\n",
    "                \n",
    "                X_train, y_train = X.loc[actual_train_idx], y.loc[actual_train_idx]\n",
    "                X_val, y_val = X.loc[actual_val_idx], y.loc[actual_val_idx]\n",
    "                \n",
    "                # Create LightGBM datasets\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "                \n",
    "                # Set early stopping parameters\n",
    "                callbacks = [\n",
    "                    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                    lgb.log_evaluation(period=0)  # Disable logging\n",
    "                ]\n",
    "                \n",
    "                # Train model\n",
    "                model = lgb.train(\n",
    "                    params, \n",
    "                    train_data,\n",
    "                    valid_sets=[train_data, val_data],\n",
    "                    valid_names=['train', 'valid'],\n",
    "                    num_boost_round=500,\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "                \n",
    "                # Make predictions\n",
    "                val_preds = model.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)  # RMSE\n",
    "                val_scores.append(val_score)\n",
    "                \n",
    "                # Store model and update feature importances\n",
    "                models.append(model)\n",
    "                importances = model.feature_importance()\n",
    "                for j, feature in enumerate(X.columns):\n",
    "                    if j < len(importances):\n",
    "                        feature_importances[feature] += importances[j] / n_splits\n",
    "                \n",
    "                print(f\"Fold {i+1}/{n_splits}: RMSE = {val_score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in fold {i+1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not models:\n",
    "        raise ValueError(\"No models were successfully trained during cross-validation\")\n",
    "    \n",
    "    # Calculate average validation score\n",
    "    avg_val_score = np.mean(val_scores)\n",
    "    print(f\"Average RMSE: {avg_val_score:.4f}\")\n",
    "    \n",
    "    # Return the best model (lowest validation error)\n",
    "    best_model_idx = np.argmin(val_scores)\n",
    "    best_model = models[best_model_idx]\n",
    "    \n",
    "    return best_model, avg_val_score, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6addcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(X: pd.DataFrame, y: pd.Series, params: Dict) -> lgb.Booster:\n",
    "    \"\"\"\n",
    "    Train a final model on all data\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target variable\n",
    "        params: Model parameters\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Create LightGBM dataset\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_revenue_prediction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run the full revenue prediction pipeline\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Revenue Prediction ===\")\n",
    "    \n",
    "    # Identify target column (using partial match since column names were cleaned)\n",
    "    target_cols = [col for col in df.columns if 'Total_Current_Operating_Revenue' in col]\n",
    "    if not target_cols:\n",
    "        raise ValueError(\"Could not find revenue target column in the dataset\")\n",
    "    target_col = target_cols[0]\n",
    "    \n",
    "    # Drop rows with missing target values\n",
    "    df_revenue = df.dropna(subset=[target_col]).copy()\n",
    "    \n",
    "    # Define columns for lag features (excluding target, identifiers, and other Y variables)\n",
    "    excluded_patterns = ['tic', 'datacqtr', 'quarter', 'year', 'time_id', 'quarter_year', \n",
    "                       'Total_Current_Operating_Revenue', 'car5']\n",
    "    \n",
    "    lag_columns = []\n",
    "    for col in df_revenue.columns:\n",
    "        if not any(pattern in col for pattern in excluded_patterns):\n",
    "            lag_columns.append(col)\n",
    "    \n",
    "    # Create lagged features (for revenue prediction, we need to lag the target variable)\n",
    "    lag_periods = [1, 2, 4]  # 1, 2, 4 quarters lag\n",
    "    lag_columns.append(target_col)\n",
    "    df_revenue = create_lagged_features(df_revenue, lag_columns, lag_periods)\n",
    "    \n",
    "    # Drop rows with NaN values for any lag features (typically the first few quarters for each ticker)\n",
    "    lag_col = f\"{target_col}_lag_1\"\n",
    "    if lag_col in df_revenue.columns:\n",
    "        df_revenue = df_revenue.dropna(subset=[lag_col]).copy()\n",
    "    \n",
    "    # Check missing values and clean data\n",
    "    _, df_revenue_cleaned = check_missing_values(df_revenue)\n",
    "    \n",
    "    # Prepare data for training\n",
    "    X, y = prepare_data_for_training(\n",
    "        df_revenue_cleaned, \n",
    "        target_col, \n",
    "        drop_cols=['tic', 'datacqtr', 'quarter_year', 'quarter', 'year']\n",
    "    )\n",
    "    \n",
    "    # Set parameters for LightGBM (simplified for faster execution)\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Train the model with cross-validation\n",
    "    cv_model, _, feature_importances = train_lightgbm_model(X, y, n_splits=3, params=params)\n",
    "    \n",
    "    # Train the final model on all data\n",
    "    final_model = train_final_model(X, y, params)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(final_model, 'models/lightgbm_revenue_model.pkl')\n",
    "    \n",
    "    # Make predictions on the entire dataset\n",
    "    df_revenue_cleaned['log_revenue_prediction'] = final_model.predict(X)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df_revenue = df_revenue_cleaned[['tic', 'quarter_year', 'log_revenue_prediction']].copy()\n",
    "    \n",
    "    # Plot feature importances\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_imp = pd.DataFrame(sorted(feature_importances.items(), key=lambda x: x[1], reverse=True), \n",
    "                                columns=['Feature', 'Importance'])\n",
    "        if len(feature_imp) > 20:\n",
    "            feature_imp = feature_imp.iloc[:20]\n",
    "        plt.barh(feature_imp['Feature'], feature_imp['Importance'])\n",
    "        plt.title('Feature Importance for Revenue Prediction')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('revenue_feature_importance.png')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not plot feature importance: {e}\")\n",
    "    \n",
    "    return output_df_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_car_prediction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run the full CAR prediction pipeline\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CAR Prediction ===\")\n",
    "    \n",
    "    # Identify target column (using partial match since column names were cleaned)\n",
    "    target_cols = [col for col in df.columns if 'car5' in col]\n",
    "    if not target_cols:\n",
    "        raise ValueError(\"Could not find CAR target column in the dataset\")\n",
    "    target_col = target_cols[0]\n",
    "    \n",
    "    # Drop rows with missing target values\n",
    "    df_car = df.dropna(subset=[target_col]).copy()\n",
    "    \n",
    "    # Define columns for lag features (excluding target, identifiers, and other Y variables)\n",
    "    excluded_patterns = ['tic', 'datacqtr', 'quarter', 'year', 'time_id', 'quarter_year', \n",
    "                        'Total_Current_Operating_Revenue', 'car5']\n",
    "    \n",
    "    lag_columns = []\n",
    "    for col in df_car.columns:\n",
    "        if not any(pattern in col for pattern in excluded_patterns):\n",
    "            lag_columns.append(col)\n",
    "    \n",
    "    # Create lagged features (for CAR, we don't need to lag the target variable)\n",
    "    lag_periods = [1, 2, 4]  # 1, 2, 4 quarters lag\n",
    "    df_car = create_lagged_features(df_car, lag_columns, lag_periods)\n",
    "    \n",
    "    # For CAR prediction, we don't need to have lagged features of the target\n",
    "    # So we don't filter based on lag features, just use the rows with non-missing CAR\n",
    "    \n",
    "    # Check missing values and clean data\n",
    "    _, df_car_cleaned = check_missing_values(df_car)\n",
    "    \n",
    "    # Prepare data for training\n",
    "    X, y = prepare_data_for_training(\n",
    "        df_car_cleaned, \n",
    "        target_col, \n",
    "        drop_cols=['tic', 'datacqtr', 'quarter_year', 'quarter', 'year']\n",
    "    )\n",
    "    \n",
    "    # Set parameters for LightGBM (simplified for faster execution)\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Train the model with cross-validation\n",
    "    cv_model, _, feature_importances = train_lightgbm_model(X, y, n_splits=3, params=params)\n",
    "    \n",
    "    # Train the final model on all data\n",
    "    final_model = train_final_model(X, y, params)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(final_model, 'models/lightgbm_car_model.pkl')\n",
    "    \n",
    "    # Make predictions on the entire dataset\n",
    "    df_car_cleaned['CAR_prediction'] = final_model.predict(X)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df_car = df_car_cleaned[['tic', 'quarter_year', 'CAR_prediction']].copy()\n",
    "    \n",
    "    # Plot feature importances\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_imp = pd.DataFrame(sorted(feature_importances.items(), key=lambda x: x[1], reverse=True), \n",
    "                                columns=['Feature', 'Importance'])\n",
    "        if len(feature_imp) > 20:\n",
    "            feature_imp = feature_imp.iloc[:20]\n",
    "        plt.barh(feature_imp['Feature'], feature_imp['Importance'])\n",
    "        plt.title('Feature Importance for CAR Prediction')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('car_feature_importance.png')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not plot feature importance: {e}\")\n",
    "    \n",
    "    return output_df_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b61ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_predictions(revenue_predictions: pd.DataFrame, car_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine revenue and CAR predictions into a single DataFrame\n",
    "    \n",
    "    Args:\n",
    "        revenue_predictions: DataFrame with revenue predictions\n",
    "        car_predictions: DataFrame with CAR predictions\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame\n",
    "    \"\"\"\n",
    "    # Merge the two DataFrames\n",
    "    combined_df = pd.merge(\n",
    "        revenue_predictions, \n",
    "        car_predictions, \n",
    "        on=['tic', 'quarter_year'], \n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Return the combined DataFrame\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data('data/model_building_data.csv')\n",
    "    \n",
    "    try:\n",
    "        # Run revenue prediction\n",
    "        revenue_predictions = run_revenue_prediction(df)\n",
    "        \n",
    "        # Run CAR prediction\n",
    "        car_predictions = run_car_prediction(df)\n",
    "        \n",
    "        # Combine predictions\n",
    "        combined_predictions = combine_predictions(revenue_predictions, car_predictions)\n",
    "        \n",
    "        # Save the final output\n",
    "        combined_predictions.to_csv('lightgbm_predictions.csv', index=False)\n",
    "        \n",
    "        print(\"\\nPipeline completed. Predictions saved to 'lightgbm_predictions.csv'\")\n",
    "        \n",
    "        return combined_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_predictions = main()\n",
    "    \n",
    "    # Display a sample of the predictions\n",
    "    if final_predictions is not None:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        print(final_predictions.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
