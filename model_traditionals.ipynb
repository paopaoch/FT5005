{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd440b7",
   "metadata": {},
   "source": [
    "# Traditional Models\n",
    "\n",
    "For: Tan Cheen Hao!!!\n",
    "\n",
    "The idea for this file is to train multiple traditional models given the dataset. The data files you will need to import is unfortunately not ready. But for now, write and test the code using `model_building_data.csv` which is provided in the data folder. Keep in mind that the final training/testing files will have more fields.\n",
    "\n",
    "You should test out everything you can think of. Focus on regression models but some classification models could be useful during stacking (up or down prediction rather than the exact value).\n",
    "\n",
    "Also, these models will not be timeseries models. Therefore, you will need to engineer lagged parameters to make use of the timeseries nature.\n",
    "\n",
    "Some models will be use during the stacking process. If not then it will be use as benchmark models. Everything will be included in the report so the more the better. Keep in mind that different models will require a different processing pipeline. This is because some models can take care of missing data, some cant. Some models are more sensitive to data ranges. Use GPT to check how to process for each! A useful tip is if you can generalise a preprocessing function for every model, then your life will be a lot easier.\n",
    "\n",
    "Last thing to keep in mind is, some rows might have missing revenue but non-missing CAR etc. If you will drop NaNs, drop for each y values differently to prevent unnecessary data loss.\n",
    "\n",
    "Tune all parameters for all models using 3-fold CV with the timesplit function like in assignment 1. I'll write a different time split function and we'll run with 5-10 fold CV again later before submission.\n",
    "\n",
    "This file should save the output of the prediction in the format:\n",
    "\n",
    "| ticker | quarter_year  | log_revenue_prediction_svm_regres | CAR_prediction_svm_regres | log_revenue_prediction_linear_regres | CAR_prediction_linear_regres |\n",
    "|--------|---------------|-----------------------------------|---------------------------|--------------------------------------|------------------------------|\n",
    "| BAC    | Q1 2001       | 123                               | 0.5                       | 123                                  | 0.5                          |\n",
    "| JPM    | Q1 2001       | 456                               | 0.8                       | 456                                  | 0.8                          |\n",
    "| WFC    | Q1 2001       | 789                               | 0.25                      | 789                                  | 0.25                         |\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238c2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Working on Target: Y2 - car5 ===\n",
      "\n",
      "\n",
      "Training linear for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "Training ridge for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "\n",
      "Training lasso for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "\n",
      "Training elasticnet for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "Training svr for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "\n",
      "Training knn for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Training decisiontree for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "Training randomforest for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "\n",
      "Training gradientboosting for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "\n",
      "Training xgboost for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "\n",
      "Training lightgbm for Y2 - car5...\n",
      "\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3548\n",
      "[LightGBM] [Info] Number of data points in the train set: 9589, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 0.004923\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Target: Y2 - car5, Model: linear\n",
      "Best Params: {}\n",
      "Scores: {'RMSE': 0.07388079737938374, 'MAE': 0.04441244029460553, 'R2': 0.012554326046703368}\n",
      "\n",
      "Target: Y2 - car5, Model: ridge\n",
      "Best Params: {'model__alpha': 100}\n",
      "Scores: {'RMSE': 0.07389829193174437, 'MAE': 0.04436794952111085, 'R2': 0.012086627647106885}\n",
      "\n",
      "Target: Y2 - car5, Model: lasso\n",
      "Best Params: {'model__alpha': 0.01}\n",
      "Scores: {'RMSE': 0.0743489720428824, 'MAE': 0.044510631948906476, 'R2': 0.0}\n",
      "\n",
      "Target: Y2 - car5, Model: elasticnet\n",
      "Best Params: {'model__alpha': 0.01, 'model__l1_ratio': 0.5}\n",
      "Scores: {'RMSE': 0.0743489720428824, 'MAE': 0.044510631948906476, 'R2': 0.0}\n",
      "\n",
      "Target: Y2 - car5, Model: svr\n",
      "Best Params: {'model__C': 0.1, 'model__epsilon': 0.01, 'model__kernel': 'linear'}\n",
      "Scores: {'RMSE': 0.07400327444925921, 'MAE': 0.044241714271493995, 'R2': 0.009277704739042436}\n",
      "\n",
      "Target: Y2 - car5, Model: knn\n",
      "Best Params: {'model__n_neighbors': 15, 'model__weights': 'uniform'}\n",
      "Scores: {'RMSE': 0.06971561452170441, 'MAE': 0.042167672093237955, 'R2': 0.12075444142993208}\n",
      "\n",
      "Target: Y2 - car5, Model: decisiontree\n",
      "Best Params: {'model__max_depth': 3, 'model__min_samples_split': 2}\n",
      "Scores: {'RMSE': 0.06844523704803258, 'MAE': 0.04405766575917876, 'R2': 0.15250620477870236}\n",
      "\n",
      "Target: Y2 - car5, Model: randomforest\n",
      "Best Params: {'model__max_depth': 5, 'model__min_samples_split': 2, 'model__n_estimators': 100}\n",
      "Scores: {'RMSE': 0.06587200837815443, 'MAE': 0.04329077526536901, 'R2': 0.21503214364001177}\n",
      "\n",
      "Target: Y2 - car5, Model: gradientboosting\n",
      "Best Params: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100}\n",
      "Scores: {'RMSE': 0.06901851070196838, 'MAE': 0.043951888491652005, 'R2': 0.13825012139717707}\n",
      "\n",
      "Target: Y2 - car5, Model: xgboost\n",
      "Best Params: {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__subsample': 1.0}\n",
      "Scores: {'RMSE': 0.07266734839871554, 'MAE': 0.0440066115927893, 'R2': 0.044724389122981045}\n",
      "\n",
      "Target: Y2 - car5, Model: lightgbm\n",
      "Best Params: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__num_leaves': 15}\n",
      "Scores: {'RMSE': 0.0731563211562789, 'MAE': 0.04409300598614666, 'R2': 0.031825189913430085}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "\n",
    "# 1. Load data\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df\n",
    "\n",
    "# 2. Preprocess data\n",
    "def preprocess_data(df, target_col, scaler_required=True):\n",
    "    df_target = df.dropna(subset=[target_col]).copy()\n",
    "    \n",
    "    X = df_target.drop(columns=[target_col])\n",
    "    y = df_target[target_col]\n",
    "\n",
    "    # Drop non-numeric columns if exist (like 'tic' or 'datacqtr')\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Handle missing values by imputing with the mean\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "    if scaler_required:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        # Sanitize feature names to remove special characters\n",
    "        sanitized_columns = [col.replace(\" \", \"_\").replace(\":\", \"_\").replace(\",\", \"_\") for col in X.columns]\n",
    "        X = pd.DataFrame(X_scaled, columns=sanitized_columns)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 3. Build model pipeline\n",
    "def build_model_pipeline(model_name):\n",
    "    models = {\n",
    "        'linear': LinearRegression(),\n",
    "        'ridge': Ridge(),\n",
    "        'lasso': Lasso(),\n",
    "        'elasticnet': ElasticNet(),\n",
    "        'svr': SVR(),\n",
    "        'knn': KNeighborsRegressor(),\n",
    "        'decisiontree': DecisionTreeRegressor(),\n",
    "        'randomforest': RandomForestRegressor(),\n",
    "        'gradientboosting': GradientBoostingRegressor(),\n",
    "        'xgboost': xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "        'lightgbm': lgb.LGBMRegressor()\n",
    "    }\n",
    "    \n",
    "    model = models.get(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 4. Hyperparameter grids\n",
    "def get_param_grid(model_name):\n",
    "    grids = {\n",
    "        'linear': {},\n",
    "        'ridge': {'alpha': [0.01, 0.1, 1, 10, 100]},\n",
    "        'lasso': {'alpha': [0.01, 0.1, 1, 10, 100]},\n",
    "        'elasticnet': {\n",
    "            'alpha': [0.01, 0.1, 1, 10],\n",
    "            'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        'svr': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'epsilon': [0.01, 0.1, 0.2],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        },\n",
    "        'knn': {\n",
    "            'n_neighbors': [3, 5, 10, 15],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        },\n",
    "        'decisiontree': {\n",
    "            'max_depth': [3, 5, 10, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'randomforest': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'gradientboosting': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'num_leaves': [15, 31, 63]\n",
    "        }\n",
    "    }\n",
    "    return grids.get(model_name, {})\n",
    "\n",
    "\n",
    "# 5. Train and tune\n",
    "def train_and_tune(X, y, model_name):\n",
    "    model = build_model_pipeline(model_name)\n",
    "    param_grid = get_param_grid(model_name)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid={'model__' + key: val for key, val in param_grid.items()},\n",
    "        cv=tscv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "# 6. Evaluate model\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "\n",
    "# 7. Main execution\n",
    "def main(filepath):\n",
    "    df = load_data(filepath)\n",
    "    \n",
    "    # Define targets (easy to extend later)\n",
    "    target_list = ['Y2 - car5']  # Add more\n",
    "\n",
    "    model_list = [\n",
    "        'linear', 'ridge', 'lasso', 'elasticnet',\n",
    "        'svr', 'knn', 'decisiontree', 'randomforest',\n",
    "        'gradientboosting', 'xgboost', 'lightgbm'\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for target in target_list:\n",
    "        print(f\"\\n=== Working on Target: {target} ===\\n\")\n",
    "        \n",
    "        X, y = preprocess_data(df, target_col=target, scaler_required=True)\n",
    "\n",
    "        for model_name in model_list:\n",
    "            print(f\"\\nTraining {model_name} for {target}...\\n\")\n",
    "            tuned_model = train_and_tune(X, y, model_name)\n",
    "            eval_metrics = evaluate_model(tuned_model.best_estimator_, X, y)\n",
    "\n",
    "            results[(target, model_name)] = {\n",
    "                'Best Params': tuned_model.best_params_,\n",
    "                'Scores': eval_metrics\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# 8. Run the script\n",
    "filepath = \"E:/Users/Walze/Downloads/data/data/model_building_data.csv\" \n",
    "results = main(filepath)\n",
    "\n",
    "# Print summary\n",
    "for (target, model_name), info in results.items():\n",
    "    print(f\"\\nTarget: {target}, Model: {model_name}\")\n",
    "    print(f\"Best Params: {info['Best Params']}\")\n",
    "    print(f\"Scores: {info['Scores']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
